#starting the clusters 
/opt/hadoop-3.3.0/sbin/start-dfs.sh && /opt/hadoop-3.3.0/sbin/start-yarn.sh
/opt/spark-3.0.1-bin-hadoop3.2/sbin/start-all.sh


# getting data
hdfs dfs -rm -r reviews
hdfs dfs -rm -r output

sqoop import --connect jdbc:mysql://13.251.71.164/BookReview?useSSL=false --table count --username root --password password
/opt/spark-3.0.1-bin-hadoop3.2/bin/spark-submit --master yarn \
                    --conf "spark.mongodb.input.uri=mongodb://46.137.200.42/test.metabooks?useSSL=false" \
                    --conf "spark.mongodb.output.uri=mongodb://46.137.200.42/test.metabooks" \
                    --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.0 \
		            /home/hadoop/db-flask/spark/mongo.py


#exporting table 
#sqoop export --connect jdbc:mysql://13.251.71.164/BookReview  --username root --password password --export-dir /user/hadoop/output   --table tfidf

#closing the clusters
/opt/spark-3.0.1-bin-hadoop3.2/sbin/stop-all.sh
/opt/hadoop-3.3.0/sbin/stop-yarn.sh && /opt/hadoop-3.3.0/sbin/stop-dfs.sh